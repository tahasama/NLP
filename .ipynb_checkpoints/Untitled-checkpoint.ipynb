{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df441421-e652-41d3-ad91-93581bb29dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (uncomment and run this line if needed)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK components\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_input(input_text):\n",
    "    # Tokenize the input text\n",
    "    tokens = word_tokenize(input_text.lower())\n",
    "    \n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d0c5e5-6251-4f5c-bace-4e2eac9c124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_intent(tokens):\n",
    "    # Define intents and corresponding keywords\n",
    "    intents = {\n",
    "        'solve_equation': {'solve', 'equation'},\n",
    "        'explain_concept': {'explain', 'concept'},\n",
    "        'ask_help': {'help', 'understand'}\n",
    "    }\n",
    "    \n",
    "    # Check for keywords in the preprocessed tokens\n",
    "    for intent, keywords in intents.items():\n",
    "        if any(keyword in tokens for keyword in keywords):\n",
    "            return intent\n",
    "    \n",
    "    # If no intent is recognized, return a default intent\n",
    "    return 'unknown_intent'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74275bf3-b77c-45df-bc50-0cce26c841c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent for query 1: solve_equation\n",
      "Intent for query 2: solve_equation\n",
      "Intent for query 3: explain_concept\n"
     ]
    }
   ],
   "source": [
    "# Sample queries to test intent recognition\n",
    "query1 = \"Can you help me understand linear equations?\"\n",
    "query2 = \"How do I solve a quadratic equation?\"\n",
    "query3 = \"Explain factoring polynomials to me.\"\n",
    "\n",
    "# Preprocess input queries\n",
    "tokens1 = preprocess_input(query1)\n",
    "tokens2 = preprocess_input(query2)\n",
    "tokens3 = preprocess_input(query3)\n",
    "\n",
    "# Recognize intents for each query\n",
    "intent1 = recognize_intent(tokens1)\n",
    "intent2 = recognize_intent(tokens2)\n",
    "intent3 = recognize_intent(tokens3)\n",
    "\n",
    "# Print recognized intents\n",
    "print(\"Intent for query 1:\", intent1)\n",
    "print(\"Intent for query 2:\", intent2)\n",
    "print(\"Intent for query 3:\", intent3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ec2d73-08a2-433a-9099-58dd44fadd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(intent):\n",
    "    # Define sample responses for each intent\n",
    "    responses = {\n",
    "        'solve_equation': \"To solve for x, isolate it on one side of the equation and perform the necessary operations.\",\n",
    "        'explain_concept': \"Sure! What concept would you like me to explain?\",\n",
    "        'ask_help': \"Of course! How can I assist you?\"\n",
    "    }\n",
    "    \n",
    "    # Retrieve the response for the recognized intent\n",
    "    response = responses.get(intent, \"I'm sorry, I'm not sure how to respond to that.\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6663d7-cce1-48dd-82e2-2223944ec55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for query 1: To solve for x, isolate it on one side of the equation and perform the necessary operations.\n",
      "Response for query 2: To solve for x, isolate it on one side of the equation and perform the necessary operations.\n",
      "Response for query 3: Sure! What concept would you like me to explain?\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for each recognized intent\n",
    "response1 = generate_response(intent1)\n",
    "response2 = generate_response(intent2)\n",
    "response3 = generate_response(intent3)\n",
    "\n",
    "# Print generated responses\n",
    "print(\"Response for query 1:\", response1)\n",
    "print(\"Response for query 2:\", response2)\n",
    "print(\"Response for query 3:\", response3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc8456-861b-49ab-81ee-4232bcab782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main interactive loop\n",
    "def chat():\n",
    "    print(\"Welcome to the Algebraic Conversational AI!\")\n",
    "    print(\"You can ask questions about algebra, and I'll do my best to help you.\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "\n",
    "        # Preprocess user input\n",
    "        tokens = preprocess_input(user_input)\n",
    "\n",
    "        # Recognize intent\n",
    "        intent = recognize_intent(tokens)\n",
    "\n",
    "        # Generate response\n",
    "        response = generate_response(intent)\n",
    "\n",
    "        # Print AI response\n",
    "        print(\"AI:\", response)\n",
    "\n",
    "        # Check if the user wants to end the conversation\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Start the conversation\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a4489-8085-497e-b101-c8c6d3cb42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define templates for different types of algebraic questions\n",
    "equation_template = \"Solve the equation {} for {}.\"\n",
    "expression_template = \"Simplify the expression {}.\"\n",
    "concept_template = \"Explain {} to me.\"\n",
    "\n",
    "# Generate synthetic questions and answers\n",
    "def generate_synthetic_data(num_questions_per_intent):\n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Generate questions for solving equations\n",
    "    for _ in range(num_questions_per_intent):\n",
    "        equation = \"2x + 3 = 9\"  # Example equation\n",
    "        variable = \"x\"\n",
    "        question = equation_template.format(equation, variable)\n",
    "        answer = \"To solve for x, subtract 3 from both sides and then divide both sides by 2.\"\n",
    "        synthetic_data.append((question, answer))\n",
    "    \n",
    "    # Generate questions for simplifying expressions\n",
    "    for _ in range(num_questions_per_intent):\n",
    "        expression = \"2x^2 + 3x - 5\"  # Example expression\n",
    "        question = expression_template.format(expression)\n",
    "        answer = \"The expression can be simplified by combining like terms.\"\n",
    "        synthetic_data.append((question, answer))\n",
    "    \n",
    "    # Generate questions for explaining concepts\n",
    "    for _ in range(num_questions_per_intent):\n",
    "        concept = \"factoring polynomials\"  # Example concept\n",
    "        question = concept_template.format(concept)\n",
    "        answer = \"Factoring polynomials involves expressing a polynomial as a product of its factors.\"\n",
    "        synthetic_data.append((question, answer))\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "# Generate synthetic data with 5 questions per intent\n",
    "synthetic_data = generate_synthetic_data(num_questions_per_intent=5)\n",
    "\n",
    "# Print the generated synthetic data\n",
    "for i, (question, answer) in enumerate(synthetic_data, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"Answer {i}: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb87736-3030-4739-b1bd-c54fe047bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (uncomment and run this line if needed)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK components\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Preprocess synthetic data\n",
    "preprocessed_synthetic_data = [(preprocess_text(question), preprocess_text(answer)) for question, answer in synthetic_data]\n",
    "\n",
    "# Print preprocessed synthetic data\n",
    "for i, (question, answer) in enumerate(preprocessed_synthetic_data, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"Answer {i}: {answer}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ee694-ae64-41e2-90c4-eb0b96411393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the preprocessed data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(preprocessed_synthetic_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of testing samples:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560712d0-0df8-4ff8-aa2e-35c5fe6229e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit vectorizer on training data and transform training and testing data\n",
    "X_train = tfidf_vectorizer.fit_transform([question for question, _ in train_data])\n",
    "X_test = tfidf_vectorizer.transform([question for question, _ in test_data])\n",
    "\n",
    "# Print the shape of the vectorized data\n",
    "print(\"Shape of training data:\", X_train.shape)\n",
    "print(\"Shape of testing data:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e36b1c-9150-405c-a008-9eb53298dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028ac50-daf2-4013-9cfc-52f65b8d851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the preprocessed data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(preprocessed_synthetic_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform([question for question, _ in train_data])\n",
    "X_test = tfidf_vectorizer.transform([question for question, _ in test_data])\n",
    "\n",
    "# Convert sparse matrices to dense NumPy arrays\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "# Extract the true answers from the training data\n",
    "true_answers_train = [answer for _, answer in train_data]\n",
    "\n",
    "# Create a dictionary mapping each unique answer to its index\n",
    "answer_to_index = {answer: idx for idx, answer in enumerate(set(true_answers_train))}\n",
    "\n",
    "# Convert true answers to their corresponding indices for both training and testing data\n",
    "train_indices = [answer_to_index[answer] for answer in true_answers_train]\n",
    "\n",
    "# Split the training data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, train_indices, val_indices = train_test_split(X_train, train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(answer_to_index), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training set and validate on the validation set\n",
    "history = model.fit(X_train, np.array(train_indices), epochs=10, batch_size=32, validation_data=(X_val, np.array(val_indices)))\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "true_answers_test = [answer for _, answer in test_data]\n",
    "test_indices = [answer_to_index.get(answer, -1) for _, answer in test_data]  # Use get() to handle missing answers\n",
    "loss, accuracy = model.evaluate(X_test, np.array(test_indices), verbose=0)\n",
    "\n",
    "# Print the model's accuracy\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acecd6-b822-425c-bb2c-6bc4862c6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's predictions on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map predicted indices back to answer labels\n",
    "predicted_answers = [list(answer_to_index.keys())[idx] for idx in predicted_indices]\n",
    "\n",
    "# Get the true labels from the testing data\n",
    "true_answers_test = [answer for _, answer in test_data]\n",
    "\n",
    "# Compare predicted and true labels\n",
    "for i in range(len(predicted_answers)):\n",
    "    print(f\"Text: {test_data[i][0]}\")\n",
    "    print(f\"True Label: {true_answers_test[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_answers[i]}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a8248-012b-4556-8ffc-645107c3db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert true labels and predicted labels to numerical indices\n",
    "true_indices = np.array([answer_to_index.get(answer, -1) for answer in true_answers_test])\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(true_indices, predicted_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaf27d-f343-49b7-b271-504ca3e4d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_indices, predicted_indices)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(set(true_answers_test)), yticklabels=sorted(set(true_answers_test)))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b1bd2-c09a-4680-9755-3aefebd941ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def custom_scorer(model, X, y):\n",
    "    y_pred = np.argmax(model.predict(X), axis=1)\n",
    "    return accuracy_score(y, y_pred)\n",
    "\n",
    "# Initialize GridSearchCV and RandomizedSearchCV with custom scoring\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring=custom_scorer)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=3, scoring=custom_scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd5494-416c-470d-8249-474e48c3b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "class KerasModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.001, dropout_rate=0.2, num_layers=2, num_units=64, epochs=10, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_layer = Input(shape=(X_train.shape[1],))\n",
    "        x = input_layer\n",
    "        x = Dense(self.num_units, activation='relu')(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            x = Dense(self.num_units, activation='relu')(x)\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "        output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(X), axis=1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        _, accuracy = self.model.evaluate(X, y)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109cedb-2ecb-48ff-885c-c3979bdb552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define a custom cross-validation strategy\n",
    "kfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV and RandomizedSearchCV with custom scoring and cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kfolds.split(X_train, train_indices), scoring=custom_scorer)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=kfolds.split(X_train, train_indices), scoring=custom_scorer)\n",
    "\n",
    "# Fit the models\n",
    "grid_search.fit(X_train, train_indices)\n",
    "random_search.fit(X_train, train_indices)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters for grid search:\", grid_search.best_params_)\n",
    "print(\"Best hyperparameters for random search:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a6480-b56f-492c-b362-c51eeebecda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
